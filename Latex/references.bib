@inproceedings{flashattention,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{flashattention2,
  title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{palu,
  title={PaLU: Compressing KV-Cache with Low-Rank Projection},
  author={Chang, Chi-Chih and Huang, Wei-Cheng and Liu, Chien-Yu and Lin, Chun-Feng and Chen, Kai-Chiang and Wu, An-Yeu},
  booktitle={EMNLP},
  year={2024}
}

@inproceedings{sparsegpt,
  title={SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={ICML},
  year={2023}
}

@inproceedings{gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and others},
  booktitle={ICLR},
  year={2023}
}

@misc{cutlass,
  title={CUTLASS: CUDA Templates for Linear Algebra Subroutines},
  author={NVIDIA},
  howpublished={\url{https://github.com/NVIDIA/cutlass}},
  year={2023}
}

@misc{llama3,
  title={Llama 3 Model Card},
  author={{Meta AI}},
  howpublished={\url{https://github.com/meta-llama/llama3}},
  year={2024}
}

@inproceedings{awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  booktitle={MLSys},
  year={2024}
}

@article{mqa,
  title={Fast Transformer Decoding: One Write-Head is All You Need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}

@inproceedings{gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  booktitle={EMNLP},
  year={2023}
}

@inproceedings{streaminglm,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  booktitle={ICLR},
  year={2024}
}

@misc{tensorrt,
  title={NVIDIA TensorRT: Programmable Inference Accelerator},
  author={NVIDIA},
  howpublished={\url{https://developer.nvidia.com/tensorrt}},
  year={2024}
}

@inproceedings{vllm,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E and Zhang, Hao and Stoica, Ion},
  booktitle={SOSP},
  year={2023}
}
